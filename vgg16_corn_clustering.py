# -*- coding: utf-8 -*-
"""VGG16-corn-clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Np7nwVeu-Z-Bx62A1DMGV7jRReB5Wc_V

Dataset Understanding & Loading

Upload Your ZIP File
"""

from google.colab import files
uploaded = files.upload()   # Select Corn_3_Classes_Image_Dataset.zip

"""Unzip the Dataset"""

import zipfile, os

ZIP_PATH = "/content/Corn_3_Classes_Image_Dataset.zip"   # ðŸ‘ˆ Correct path
DATA_DIR = "/content/corn_dataset"

os.makedirs(DATA_DIR, exist_ok=True)

with zipfile.ZipFile(ZIP_PATH, "r") as z:
    z.extractall(DATA_DIR)

print("Extracted to:", DATA_DIR)

"""List Classes & Image Counts"""

import os, glob

root = os.path.join(DATA_DIR, "Corn_3_Classes_Image_Dataset")
classes = [d for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))]
counts = {c: len(glob.glob(os.path.join(root, c, "*.*"))) for c in classes}

print("Classes found:", classes)
for c, n in counts.items():
    print(f"{c}: {n} images")

"""Display Sample Images (3 per class)"""

import random
from PIL import Image
import matplotlib.pyplot as plt

def show_samples(root, classes, samples_per_class=3):
    fig_rows = len(classes)
    fig, axes = plt.subplots(fig_rows, samples_per_class, figsize=(samples_per_class*4, fig_rows*4))

    if fig_rows == 1:
        axes = [axes]

    for i, c in enumerate(classes):
        imgs = glob.glob(os.path.join(root, c, "*.*"))
        chosen = random.sample(imgs, min(samples_per_class, len(imgs)))

        for j in range(samples_per_class):
            ax = axes[i][j] if fig_rows > 1 else axes[j]
            ax.axis("off")

            if j < len(chosen):
                img = Image.open(chosen[j]).convert("RGB")
                ax.imshow(img)
                ax.set_title(c, fontsize=9)
            else:
                ax.set_visible(False)

    plt.tight_layout()
    plt.show()

show_samples(root, classes)

"""Prepare Consistent Image Size (Resize to 224Ã—224)"""

from PIL import Image

OUT_DIR = "/content/corn_processed"
TARGET_SIZE = (224, 224)

os.makedirs(OUT_DIR, exist_ok=True)

def resize_image(img, target_size):
    return img.resize(target_size, Image.BILINEAR)

for c in classes:
    src_dir = os.path.join(root, c)
    dst_dir = os.path.join(OUT_DIR, c)
    os.makedirs(dst_dir, exist_ok=True)

    for f in glob.glob(os.path.join(src_dir, "*.*")):
        try:
            img = Image.open(f).convert("RGB")
            img = resize_image(img, TARGET_SIZE)
            img.save(os.path.join(dst_dir, os.path.basename(f)))
        except:
            print("Error:", f)

print("Processed images saved to:", OUT_DIR)

"""Verify Processed Image Counts"""

processed_counts = {c: len(glob.glob(os.path.join(OUT_DIR, c, "*.*"))) for c in classes}
processed_counts

"""Step 2 â€” Feature Extraction (Multiple Methods)

Cell 0 â€” common imports & gather files
"""

import os, glob, numpy as np
from PIL import Image

DATA_DIR = "/content/corn_processed"   # change if needed
classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])

# list (filepath, label) pairs
def gather_files(data_dir, classes):
    files, labels = [], []
    for c in classes:
        fps = sorted(glob.glob(os.path.join(data_dir, c, "*.*")))
        files += fps
        labels += [c] * len(fps)
    return files, labels

files, labels = gather_files(DATA_DIR, classes)
print("classes:", classes)
print("total images:", len(files))

"""Cell A â€” Color histograms (RGB, 8 bins per channel)"""

import numpy as np

def rgb_histogram(img, bins_per_channel=8):
    arr = np.array(img)
    h=[]
    for ch in range(3):
        hist, _ = np.histogram(arr[:,:,ch], bins=bins_per_channel, range=(0,256))
        h.append(hist.astype(float))
    h = np.concatenate(h)
    h /= (h.sum() + 1e-9)
    return h

bins=8
color_feats = [ rgb_histogram(Image.open(fp).convert("RGB").resize((224,224)), bins) for fp in files ]
color_feats = np.vstack(color_feats)
np.save("/content/color_hists.npy", color_feats)
print("color_hists saved, shape:", color_feats.shape)

"""Cell B â€” LBP texture features (skimage)"""

# install skimage if not present: !pip install scikit-image
from skimage.feature import local_binary_pattern
from skimage.color import rgb2gray
import numpy as np

P, R, METHOD = 8, 1, 'uniform'
n_bins = P + 2

def lbp_hist(img):
    gray = np.array(rgb2gray(np.array(img)))
    lbp = local_binary_pattern(gray, P, R, method=METHOD)
    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))
    hist = hist.astype(float)
    hist /= (hist.sum() + 1e-9)
    return hist

lbp_feats = [ lbp_hist(Image.open(fp).convert("RGB").resize((224,224))) for fp in files ]
lbp_feats = np.vstack(lbp_feats)
np.save("/content/lbp_feats.npy", lbp_feats)
print("lbp_feats saved, shape:", lbp_feats.shape)

"""Cell C â€” Deep features (VGG16 pooling='avg' â†’ 512-d)"""

# install tf in Colab if needed; Colab usually has TF preinstalled
import numpy as np
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model

model = VGG16(weights='imagenet', include_top=False, pooling='avg')  # 512-d

def to_model_array(fp, target_size=(224,224)):
    img = image.load_img(fp, target_size=target_size)
    arr = image.img_to_array(img)
    arr = np.expand_dims(arr, 0)
    arr = preprocess_input(arr)
    return arr

BATCH = 32
deep_feats = []
for i in range(0, len(files), BATCH):
    batch = files[i:i+BATCH]
    batch_arr = np.vstack([to_model_array(fp) for fp in batch])
    feats = model.predict(batch_arr, verbose=0)
    deep_feats.append(feats)
deep_feats = np.vstack(deep_feats)
np.save("/content/deep_feats.npy", deep_feats)
print("deep_feats saved, shape:", deep_feats.shape)

"""Combine & standardize features (choose combination)"""

from sklearn.preprocessing import StandardScaler
import numpy as np

color = np.load("/content/color_hists.npy")    # (N, 24)
lbp   = np.load("/content/lbp_feats.npy")      # (N, P+2)
deep  = np.load("/content/deep_feats.npy")     # (N, 512)

# Options: use only deep, or deep+color, or all three.
X = np.hstack([deep, color, lbp])   # change as desired

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
np.save("/content/features_combined.npy", X_scaled)
np.save("/content/meta.npy", np.array([files, labels], dtype=object))
print("features_combined saved, shape:", X_scaled.shape)

"""Step 3 â€” Dimensionality Reduction

Load features & labels
"""

import numpy as np

X = np.load("/content/features_combined.npy")     # (N, D)
files, labels = np.load("/content/meta.npy", allow_pickle=True)
unique_labels = sorted(list(set(labels)))

print("Feature shape:", X.shape)
print("Classes:", unique_labels)

"""A) PCA (2D & 3D)
â€” PCA 2D
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca2 = PCA(n_components=2)
X_pca2 = pca2.fit_transform(X)
print("PCA2 shape:", X_pca2.shape)

plt.figure(figsize=(7,6))
for c in unique_labels:
    idx = [i for i, lab in enumerate(labels) if lab == c]
    plt.scatter(X_pca2[idx,0], X_pca2[idx,1], label=c, s=10)
plt.title("PCA (2D)")
plt.legend()
plt.show()

"""â€” PCA 3D"""

from mpl_toolkits.mplot3d import Axes3D

pca3 = PCA(n_components=3)
X_pca3 = pca3.fit_transform(X)

fig = plt.figure(figsize=(7,6))
ax = fig.add_subplot(111, projection='3d')
for c in unique_labels:
    idx = [i for i, lab in enumerate(labels) if lab == c]
    ax.scatter(X_pca3[idx,0], X_pca3[idx,1], X_pca3[idx,2], label=c, s=10)
ax.set_title("PCA (3D)")
plt.legend()
plt.show()

"""B) t-SNE (2D)"""

from sklearn.manifold import TSNE

# First reduce to 50 dimensions (recommended for speed & stability)
pca50 = PCA(n_components=50)
X_50 = pca50.fit_transform(X)

tsne2 = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca')
X_tsne2 = tsne2.fit_transform(X_50)
print("t-SNE shape:", X_tsne2.shape)

plt.figure(figsize=(7,6))
for c in unique_labels:
    idx = [i for i, lab in enumerate(labels) if lab == c]
    plt.scatter(X_tsne2[idx,0], X_tsne2[idx,1], label=c, s=10)
plt.title("t-SNE (2D)")
plt.legend()
plt.show()

"""C) UMAP (2D)"""

!pip install umap-learn

import umap.umap_ as umap

umap2 = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')
X_umap2 = umap2.fit_transform(X)

plt.figure(figsize=(7,6))
for c in unique_labels:
    idx = [i for i, lab in enumerate(labels) if lab == c]
    plt.scatter(X_umap2[idx,0], X_umap2[idx,1], label=c, s=10)
plt.title("UMAP (2D)")
plt.legend()
plt.show()

"""Save reduced features for clustering"""

np.save("/content/pca2.npy", X_pca2)
np.save("/content/pca3.npy", X_pca3)
np.save("/content/tsne2.npy", X_tsne2)
np.save("/content/umap2.npy", X_umap2)

print("Saved PCA, t-SNE, UMAP embeddings.")

"""Clustering (Multiple Algorithms)

common imports + load data
"""

import numpy as np, os, random
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

X = np.load("/content/features_combined.npy")          # (N, D)
files, labels = np.load("/content/meta.npy", allow_pickle=True)
files = list(files)
labels = list(labels)

le = LabelEncoder()
y_true = le.fit_transform(labels)   # numeric ground-truth for ARI/NMI
classes = le.classes_
print("X shape:", X.shape, "N:", len(files), "classes:", list(classes))

"""helper funcs (metrics + quick 2D plotting)"""

from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score
import pandas as pd

def eval_and_report(name, labels_pred, X_for_sil=None):
    if X_for_sil is None:
        X_for_sil = X
    # metrics
    n_clusters = len(set(labels_pred)) - (1 if -1 in labels_pred else 0)
    sil = silhouette_score(X_for_sil, labels_pred) if n_clusters>1 else float("nan")
    ari = adjusted_rand_score(y_true, labels_pred)
    nmi = normalized_mutual_info_score(y_true, labels_pred)
    return {"method": name, "n_clusters": n_clusters, "silhouette": sil, "ARI": ari, "NMI": nmi}

def plot_2d(emb2, labels_pred, title, size=20):
    plt.figure(figsize=(6,5))
    for lab in np.unique(labels_pred):
        idx = np.where(labels_pred==lab)[0]
        if lab == -1:
            lab_name = "outlier"
        else:
            lab_name = f"cluster {lab}"
        plt.scatter(emb2[idx,0], emb2[idx,1], s=8, label=lab_name)
    plt.legend(markerscale=2, fontsize=8)
    plt.title(title)
    plt.tight_layout()
    plt.show()

# ensure we have a 2D embedding for plotting (PCA 2)
try:
    emb2 = np.load("/content/pca2.npy")
except:
    from sklearn.decomposition import PCA
    emb2 = PCA(n_components=2).fit_transform(X)
    np.save("/content/pca2.npy", emb2)

"""K-means (k=3)"""

from sklearn.cluster import KMeans

k = 3
km = KMeans(n_clusters=k, n_init=20, random_state=42)
pred_km = km.fit_predict(X)

# reports
row_km = eval_and_report("KMeans_k=3", pred_km)
row_km["inertia"] = km.inertia_
print(row_km)

# plot
plot_2d(emb2, pred_km, "KMeans (k=3)")

"""Hierarchical Clustering (Ward) + dendrogram (sampled for speed)"""

from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

# For dendrogram plotting use a random subset (e.g., 200) so it's readable
subset = 200 if len(X) > 200 else len(X)
idx_subset = random.sample(range(len(X)), subset)
Z = linkage(X[idx_subset], method="ward")   # can be slow on very large N

plt.figure(figsize=(10,4))
dendrogram(Z, truncate_mode='level', p=7, no_labels=True)
plt.title("Hierarchical clustering dendrogram (sample)")
plt.show()

# Now compute full clustering labels via fcluster (k=3)
Z_full = linkage(X, method="ward")
pred_hier = fcluster(Z_full, t=3, criterion='maxclust') - 1   # make 0-based
row_hier = eval_and_report("Hierarchical_Ward_k=3", pred_hier)
print(row_hier)
plot_2d(emb2, pred_hier, "Hierarchical (Ward) k=3")

"""DBSCAN (density-based)"""

from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
import numpy as np

# Choose eps by looking at k-distance plot; we will compute k=5 distances
nbrs = NearestNeighbors(n_neighbors=5).fit(X)
distances, _ = nbrs.kneighbors(X)
k_dist = np.sort(distances[:, -1])
# (Optional) plot k-distance curve to help choose eps
plt.figure(figsize=(6,3))
plt.plot(k_dist)
plt.title("k-distance (k=5) sorted - use elbow as eps guide")
plt.ylabel("distance")
plt.xlabel("points sorted")
plt.tight_layout()
plt.show()

# sensible defaults (you can tune eps and min_samples)
db = DBSCAN(eps=0.9, min_samples=6, n_jobs=-1)   # change eps if needed
pred_db = db.fit_predict(X)
row_db = eval_and_report("DBSCAN", pred_db, X_for_sil=X)
print(row_db)
plot_2d(emb2, pred_db, f"DBSCAN (eps={db.eps}, min_samples={db.min_samples})")

"""Gaussian Mixture Model (GMM) â€” soft clustering"""

from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm_labels = gmm.fit_predict(X)   # hard assignment (MAP)
probs = gmm.predict_proba(X)
row_gmm = eval_and_report("GMM_k=3", gmm_labels)
print(row_gmm)
plot_2d(emb2, gmm_labels, "GMM (k=3)")

"""Spectral Clustering (k=3)"""

from sklearn.cluster import SpectralClustering

spec = SpectralClustering(n_clusters=3, random_state=42, assign_labels='kmeans', n_init=10, affinity='nearest_neighbors')
spec_labels = spec.fit_predict(X)
row_spec = eval_and_report("Spectral_k=3", spec_labels)
print(row_spec)
plot_2d(emb2, spec_labels, "Spectral Clustering k=3")

"""Summary table of results"""

import pandas as pd

rows = [row_km, row_hier, row_db, row_gmm, row_spec]
df = pd.DataFrame(rows)
# reorder columns nicely
cols = ["method", "n_clusters", "inertia", "silhouette", "ARI", "NMI"]
for c in cols:
    if c not in df.columns:
        df[c] = np.nan
df = df[cols]
display(df.sort_values("ARI", ascending=False).reset_index(drop=True))

"""Step 5 â€” Evaluation of Clusters"""

pred_km        # K-means cluster labels
y_true         # ground truth class IDs
classes        # class names
X              # feature matrix (original scaled)
emb2           # PCA 2D embedding (optional)

"""Load everything"""

import numpy as np
from sklearn.preprocessing import LabelEncoder

# Load features & ground truth
X = np.load("/content/features_combined.npy")
files, labels = np.load("/content/meta.npy", allow_pickle=True)

le = LabelEncoder()
y_true = le.fit_transform(labels)
classes = le.classes_
print("Loaded:", X.shape, "labels:", classes)

"""Load clustering results"""

from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, n_init=20, random_state=42)
pred_km = km.fit_predict(X)

print("KMeans labels:", np.unique(pred_km))

"""Internal Evaluation Metrics"""

from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

silhouette = silhouette_score(X, pred_km)
davies     = davies_bouldin_score(X, pred_km)
calinski   = calinski_harabasz_score(X, pred_km)

print("=== Internal Metrics ===")
print("Silhouette Score:        ", round(silhouette, 4))
print("Daviesâ€“Bouldin Index:    ", round(davies, 4))
print("Calinskiâ€“Harabasz Score: ", round(calinski, 4))

"""External Evaluation Metrics (for analysis only)"""

from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

ari = adjusted_rand_score(y_true, pred_km)
nmi = normalized_mutual_info_score(y_true, pred_km)

print("\n=== External Metrics ===")
print("Adjusted Rand Index (ARI):", round(ari, 4))
print("Normalized Mutual Info (NMI):", round(nmi, 4))

"""Confusion Matrix"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, pred_km)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=[f"Cluster {i}" for i in range(3)],
            yticklabels=classes)
plt.title("Confusion Matrix â€” True vs Cluster")
plt.xlabel("Predicted Cluster")
plt.ylabel("True Class")
plt.show()

cm

"""Cluster-wise accuracy"""

import pandas as pd

cluster_mapping = {}
for c in range(3):
    idx = np.where(pred_km == c)[0]
    true_labels = y_true[idx]
    majority = np.bincount(true_labels).argmax()
    cluster_mapping[c] = classes[majority]

df_map = pd.DataFrame({
    "cluster": list(cluster_mapping.keys()),
    "dominant_class": list(cluster_mapping.values())
})
df_map

"""Step 6 â€” Visualization

Setup & load data
"""

import os, random, numpy as np
from PIL import Image
import matplotlib.pyplot as plt

OUT_DIR = "/content/visualizations"
os.makedirs(OUT_DIR, exist_ok=True)

X = np.load("/content/features_combined.npy")
files, labels = np.load("/content/meta.npy", allow_pickle=True)
files = list(files); labels = list(labels)
print("Loaded:", X.shape, "images:", len(files))

"""PCA 2D plot (save + show)"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
X_pca2 = pca.fit_transform(X)

plt.figure(figsize=(7,6))
for cl in sorted(set(range(3))):                        # assumes k=3 clusters
    idx = (pred_km == cl) if 'pred_km' in globals() else None
    if idx is None:
        # compute KMeans if pred_km not present
        from sklearn.cluster import KMeans
        pred_km = KMeans(n_clusters=3, n_init=20, random_state=42).fit_predict(X)
        idx = (pred_km == cl)
    plt.scatter(X_pca2[idx,0], X_pca2[idx,1], s=10, label=f"cluster {cl}")
plt.legend(fontsize=8)
plt.title("PCA (2D) - clusters")
plt.xlabel("PC1"); plt.ylabel("PC2")
plt.tight_layout()
p = os.path.join(OUT_DIR, "pca_clusters.png")
plt.savefig(p, dpi=150)
plt.show()
print("Saved:", p)

"""t-SNE 2D plot (PCA50 â†’ t-SNE) â€” faster & stable"""

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

pca50 = PCA(n_components=min(50, X.shape[1]))
X_50 = pca50.fit_transform(X)
tsne = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42)
X_tsne2 = tsne.fit_transform(X_50)

plt.figure(figsize=(7,6))
for cl in sorted(set(pred_km)):
    idx = (pred_km == cl)
    plt.scatter(X_tsne2[idx,0], X_tsne2[idx,1], s=10, label=f"cluster {cl}")
plt.legend(fontsize=8)
plt.title("t-SNE (2D) - clusters")
plt.xlabel("t-SNE-1"); plt.ylabel("t-SNE-2")
plt.tight_layout()
p = os.path.join(OUT_DIR, "tsne_clusters.png")
plt.savefig(p, dpi=150)
plt.show()
print("Saved:", p)

"""UMAP 2D plot"""

# install only if you want to run UMAP in Colab:
# !pip install --quiet umap-learn

try:
    import umap.umap_ as umap
    umapper = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42)
    X_umap2 = umapper.fit_transform(X)

    plt.figure(figsize=(7,6))
    for cl in sorted(set(pred_km)):
        idx = (pred_km == cl)
        plt.scatter(X_umap2[idx,0], X_umap2[idx,1], s=10, label=f"cluster {cl}")
    plt.legend(fontsize=8)
    plt.title("UMAP (2D) - clusters")
    plt.xlabel("UMAP-1"); plt.ylabel("UMAP-2")
    plt.tight_layout()
    p = os.path.join(OUT_DIR, "umap_clusters.png")
    plt.savefig(p, dpi=150)
    plt.show()
    print("Saved:", p)
except Exception as e:
    print("UMAP not available or failed:", e)

"""Sample images from each cluster (grid) â€” saved per cluster"""

from math import ceil
samples_per_cluster = 6
for cl in sorted(set(pred_km)):
    idxs = np.where(pred_km == cl)[0].tolist()
    chosen = random.sample(idxs, min(samples_per_cluster, len(idxs)))
    cols = samples_per_cluster
    rows = 1
    plt.figure(figsize=(cols*2, rows*2))
    for i, idx in enumerate(chosen):
        ax = plt.subplot(rows, cols, i+1)
        ax.axis('off')
        img = Image.open(files[idx]).convert("RGB").resize((224,224))
        plt.imshow(img)
        plt.title(f"idx {idx}", fontsize=8)
    # hide remaining axes
    for j in range(len(chosen), cols):
        ax = plt.subplot(rows, cols, j+1)
        ax.axis('off')
    plt.suptitle(f"Samples - cluster {cl}")
    plt.tight_layout(rect=[0,0,1,0.95])
    p = os.path.join(OUT_DIR, f"samples_cluster_{cl}.png")
    plt.savefig(p, dpi=150)
    plt.show()
    print("Saved:", p)

"""Representative (centroid) images â€” nearest image to each KMeans centroid"""

import numpy as np
from sklearn.cluster import KMeans

# recompute KMeans if pred_km not present
if 'pred_km' not in globals():
    km = KMeans(n_clusters=3, n_init=20, random_state=42).fit(X)
    pred_km = km.labels_
    centers = km.cluster_centers_
else:
    # get centers from KMeans fit above, or recompute to be safe
    km = KMeans(n_clusters=3, n_init=20, random_state=42).fit(X)
    centers = km.cluster_centers_

for cl in range(centers.shape[0]):
    center = centers[cl]
    dists = np.linalg.norm(X - center, axis=1)
    idx_near = int(np.argmin(dists))
    img = Image.open(files[idx_near]).convert("RGB").resize((224,224))
    p = os.path.join(OUT_DIR, f"centroid_cluster_{cl}.png")
    img.save(p)
    display(img)
    print(f"Cluster {cl} representative (idx {idx_near}) saved to:", p)

"""Dendrogram (hierarchical) â€” sample subset for readability"""

from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt
import random

subset = 200 if len(X) > 200 else len(X)
idx_subset = random.sample(range(len(X)), subset)
Z = linkage(X[idx_subset], method="ward")

plt.figure(figsize=(10,4))
dendrogram(Z, truncate_mode='level', p=7, no_labels=True)
plt.title("Hierarchical linkage dendrogram (sample)")
plt.tight_layout()
p = os.path.join(OUT_DIR, "dendrogram_sample.png")
plt.savefig(p, dpi=150)
plt.show()
print("Saved:", p)

"""Quick: list saved files"""

import os, glob
files_saved = sorted(glob.glob(os.path.join(OUT_DIR, "*")))
print("Saved visualizations:")
for f in files_saved:
    print(f)