ğŸŒ½ Corn Kernel Clustering â€” Unsupervised Machine Learning Pipeline

This project performs unsupervised clustering on corn kernel images using a complete computer vision + machine learning pipeline.
It extracts visual features (color, texture, deep learning), applies dimensionality reduction (PCA, t-SNE, UMAP), runs multiple clustering algorithms, and evaluates how well the model discovers the three corn varieties.

ğŸ“ Dataset

The dataset contains 1,050 images of three Zea mays varieties:

Zea_mays_Chulpi_Cancha

Zea_mays_Indurata

Zea_mays_Rugosa

Each class has ~350 RGB images.

Example dataset ZIP path used during development:

/mnt/data/Corn_3_Classes_Image_Dataset.zip

ğŸ”§ Project Structure
corn-clustering/
â”‚
â”œâ”€â”€ main.py                # complete pipeline
â”œâ”€â”€ requirements.txt       # install dependencies
â”œâ”€â”€ README.md              # project description
â”œâ”€â”€ .gitignore             # ignore cache & generated files
â”‚
â”œâ”€â”€ results.jpeg             
â””â”€â”€ visualizations.jpeg

ğŸš€ Pipeline Overview

This project contains six core steps, all implemented in main.py.

1. Preprocessing

Unzip dataset

Resize images to 224Ã—224

Convert to RGB

Store processed images

Libraries Used:
PIL, glob, os

2. Feature Extraction

Three complementary feature types were extracted:

âœ” Color Histograms

RGB histogram (8 bins Ã— 3 channels)

Captures overall color tone

numpy.histogram()

âœ” Local Binary Patterns (LBP)

Captures texture micro-patterns

skimage.feature.local_binary_pattern()

âœ” VGG16 Deep Features (Best Performers)

Pretrained CNN on ImageNet

512-dimensional embeddings

tensorflow.keras.applications.vgg16

3. Dimensionality Reduction
âœ” PCA

Linear reduction

Helps visualize clusters in 2D/3D

âœ” t-SNE

Non-linear visualization

Reveals natural grouping

âœ” UMAP

Faster, preserves structure well

Libraries: sklearn, umap-learn

4. Clustering Algorithms

Multiple clustering algorithms were applied and compared:

K-Means (Best performer)

Gaussian Mixture Models (GMM)

Hierarchical Clustering

DBSCAN

Spectral Clustering

Libraries: sklearn.cluster, scipy.cluster.hierarchy

5. Evaluation Metrics
Internal Metrics

Evaluate clustering without labels:

Metric	Meaning
Silhouette Score	Separation quality
Daviesâ€“Bouldin Index	Cluster compactness
Calinskiâ€“Harabasz Score	Variance ratio
External Metrics

(Used only for analysis, not training)

Metric	Meaning
ARI	matches predicted vs. true labels
NMI	shared information
Confusion Matrix

Shows cluster â†” true class alignment.

6. Visualization

Figures include:

PCA clusters

t-SNE clusters

UMAP clusters

Sample images per cluster

Centroid (representative) images

Hierarchical dendrogram

All plots saved to /visualizations/.

â­ Results Summary
Algorithm	ARI	NMI
K-Means	â‰ˆ 0.991	â‰ˆ 0.983
GMM	â‰ˆ 0.991	â‰ˆ 0.983
Spectral	â‰ˆ 0.988	â‰ˆ 0.979
Hierarchical	â‰ˆ 0.972	â‰ˆ 0.954
DBSCAN	Very poor	Very poor

âœ” Deep VGG16 embeddings + KMeans achieved near-perfect unsupervised clustering.
âœ” Clear separation in PCA, t-SNE, and UMAP visualizations.
âœ” Confusion matrix shows clusters almost perfectly match real classes.

ğŸ§ª How to Run
1. Install dependencies
pip install -r requirements.txt

2. Place dataset ZIP in data/ or anywhere local.
3. Run the full pipeline
python main.py


Outputs will be saved in:

/results/

/visualizations/

ğŸ“Œ Technologies Used

Python

NumPy

Scikit-Learn

SciPy

Matplotlib

Scikit-Image

TensorFlow / Keras

UMAP

ğŸ“„ License

 MIT License
